{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61939a28-2b82-45ba-91d5-2295b13f5a18",
   "metadata": {},
   "source": [
    "### Specify the path to the dataset you want to modfiy and where to save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad33b68f-e0e4-4c44-9c54-98e19e1b1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to run the helper function cells first\n",
    "# dataset_path = \"/data/mm12191/datasets/dataset_batch550000-838143.pkl\"\n",
    "dataset_path = \"/data/kb4083/datasets/sample_20m_test_even_smaller.pkl\"\n",
    "# path where the modified dataset should be saved and\n",
    "#the name of file without the extension\n",
    "save_path = \"/data/kb4083/datasets/sample_20m_test_even_smaller_modified\"\n",
    "new_extension = \"pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91101da1-342d-469f-99ce-f569a8347bfe",
   "metadata": {},
   "source": [
    "### Read the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c415466c-5ef7-473f-986d-b14b7b0bb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "# in case the dataset was stored in the json format\n",
    "if dataset_path.endswith(\"json\"):\n",
    "    # we open the dataset as a normal file\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dataset_str = f.read()\n",
    "    programs_dict = json.loads(dataset_str)\n",
    "# in case the dataset was stored in the pkl format\n",
    "elif dataset_path.endswith(\"pkl\"):\n",
    "    # we un-pickle the file using the pickle library \n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        programs_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f546a8d-0a36-470d-b94b-68fa7a6d613c",
   "metadata": {},
   "source": [
    "### Apply modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f102d2-ed1c-445f-b0df-c40e5a2bc688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:05<00:00, 16.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# shuffle the programs of the dataset\n",
    "# Maximum number of nested loops in a program\n",
    "max_depth = 5\n",
    "# Number of all the dropped schedules\n",
    "nb_dropped = 0\n",
    "nb_dropped_random_matrix = 0\n",
    "# Number of dropped schedules due to the drop schedule function only\n",
    "nb_pruned = 0\n",
    "# List of dropped functions \n",
    "dropped_funcs = []\n",
    "\n",
    "functions_list = list(programs_dict.keys())\n",
    "new_programs_dict = {}\n",
    "for index, function_name in enumerate(tqdm(functions_list)):\n",
    "    # Check whether this function should be dropped\n",
    "    if drop_program(programs_dict[function_name], function_name):\n",
    "        nb_dropped += len(\n",
    "            programs_dict[function_name][\"schedules_list\"]\n",
    "        )\n",
    "        dropped_funcs.append(function_name)\n",
    "        continue\n",
    "\n",
    "\n",
    "    program_json = programs_dict[function_name][\"program_annotation\"]\n",
    "    \n",
    "    # Adding expression representation\n",
    "    program_json = add_expression_representation_to_function(program_json, function_name)\n",
    "\n",
    "    # Check whether the whole function should be dropped because of the number of loops and accesses\n",
    "    try:\n",
    "        check_program_access_and_depth(\n",
    "            programs_dict[function_name],\n",
    "            max_depth=max_depth,\n",
    "        )\n",
    "    except (NbAccessException, LoopsDepthException):\n",
    "        # If one of the two exceptions was raised, we drop all the schedules for that program and skip to the next program.\n",
    "        nb_dropped += len(\n",
    "            programs_dict[function_name][\"schedules_list\"]\n",
    "        )\n",
    "        continue\n",
    "    # Get the initial execution time for the program to calculate the speedups (initial exec time / transformed exec time)\n",
    "    program_exec_time = programs_dict[function_name][\n",
    "        \"initial_execution_time\"\n",
    "    ]\n",
    "\n",
    "    new_programs_dict[function_name] = programs_dict[function_name].copy()\n",
    "    new_programs_dict[function_name][\"schedules_list\"] = []\n",
    "    \n",
    "    for iterator in new_programs_dict[function_name][\"program_annotation\"][\"iterators\"]:\n",
    "        new_programs_dict[function_name][\"program_annotation\"][\"iterators\"][iterator][\"upper_bound\"] = str(new_programs_dict[function_name][\"program_annotation\"][\"iterators\"][iterator][\"upper_bound\"])\n",
    "        new_programs_dict[function_name][\"program_annotation\"][\"iterators\"][iterator][\"lower_bound\"] = str(new_programs_dict[function_name][\"program_annotation\"][\"iterators\"][iterator][\"lower_bound\"])\n",
    "        \n",
    "    # For each schedule (sequence of transformations) collected for this function\n",
    "    for schedule_index in range( len(programs_dict[function_name][\"schedules_list\"])):\n",
    "\n",
    "        # Get the schedule JSON representation\n",
    "        schedule_json = programs_dict[function_name][\"schedules_list\"][schedule_index]\n",
    "\n",
    "        # Get the transformed execution timeschedule_index\n",
    "        sched_exec_time = np.min(schedule_json[\"execution_times\"])\n",
    "\n",
    "        # Check if this schedule should be dropped\n",
    "        if drop_schedule(programs_dict[function_name], schedule_index ) or (not sched_exec_time):\n",
    "            nb_dropped += 1\n",
    "            nb_pruned += 1\n",
    "            continue\n",
    "        # Calculate the speed up obtained from applying the list of transformations spesified by the schedule\n",
    "        sched_speedup = program_exec_time / sched_exec_time\n",
    "\n",
    "        # Check whether we can set a default value for this speedup through the can_set_default_eval function.\n",
    "        def_sp = can_set_default_eval(program_json, schedule_json)\n",
    "\n",
    "\n",
    "        # If the function returns 0, this means no default value was spesified\n",
    "        if def_sp > 0:\n",
    "            sched_speedup = def_sp\n",
    "            \n",
    "\n",
    "        # Check whether we can clip the obtained speedup\n",
    "        sched_speedup = speedup_clip(sched_speedup)\n",
    "        \n",
    "        # TODO add speedupclip\n",
    "\n",
    "        # Fill the obtained template with the corresponsing schedule features\n",
    "        try:\n",
    "            comp_transformations = get_transformations_list(\n",
    "                program_json,\n",
    "                schedule_json,\n",
    "                max_depth,\n",
    "            )\n",
    "        except NbTranformationException:\n",
    "            # If the number of transformations exceeded the specified max, we skip this schedule\n",
    "            nb_dropped += 1\n",
    "            continue\n",
    "\n",
    "        except RandomMatrix :\n",
    "            nb_dropped += 1\n",
    "            nb_dropped_random_matrix += 1\n",
    "            continue\n",
    "        computations_dict = program_json[\"computations\"]\n",
    "        ordered_comp_list = sorted(\n",
    "            list(computations_dict.keys()),\n",
    "            key=lambda x: computations_dict[x][\"absolute_order\"],\n",
    "        )\n",
    "        \n",
    "        schedule_json[\"legality_check\"] = True\n",
    "        schedule_json[\"exploration_method\"] = 1\n",
    "        for comp_index, comp_name in enumerate(ordered_comp_list):\n",
    "            if(\"interchange_dims\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"interchange_dims\", None)\n",
    "            \n",
    "            if(\"unfuse_iterators\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"unfuse_iterators\", None)\n",
    "                \n",
    "            if(\"average_skewed_extents\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"average_skewed_extents\", None)\n",
    "                \n",
    "            if(\"skewing\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"skewing\", None)\n",
    "\n",
    "            if(\"Transformation Matrix\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"Transformation Matrix\", None)\n",
    "\n",
    "            if(\"transformation_matrix\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"transformation_matrix\", None)\n",
    "\n",
    "            if(\"transformation_matrices\" in schedule_json[comp_name]):\n",
    "                remove_key = schedule_json[comp_name].pop(\"transformation_matrices\", None)\n",
    "                \n",
    "            if(\"shiftings\" not in schedule_json[comp_name]):\n",
    "                schedule_json[comp_name][\"shiftings\"] = None\n",
    "                \n",
    "            if(abs(comp_transformations[comp_index]).sum() != 0):\n",
    "                schedule_json[comp_name][\"transformations_list\"] = comp_transformations[comp_index].tolist()[::-1]\n",
    "            else:\n",
    "                schedule_json[comp_name][\"transformations_list\"] = []\n",
    "            \n",
    "        schedule_json[\"sched_str\"] = get_schedule_str(program_json, schedule_json)\n",
    "        new_programs_dict[function_name][\"schedules_list\"].append(schedule_json)\n",
    "        \n",
    "    # Adding roots table to the program tree structure\n",
    "    if \"roots\" not in programs_dict[function_name][\"schedules_list\"][0][\"tree_structure\"]:\n",
    "        for i in range(len(programs_dict[function_name][\"schedules_list\"])):\n",
    "            programs_dict[function_name][\"schedules_list\"][i][\"tree_structure\"] = {\"roots\": [programs_dict[function_name][\"schedules_list\"][i][\"tree_structure\"]]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbaa714-dc20-4bb2-b709-bf12922efd6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8217ea-faab-4e63-8214-09ba5312458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = save_path +\".\"+new_extension\n",
    "if(new_extension == \"json\"):\n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(new_programs_dict, outfile)\n",
    "if(new_extension == \"pkl\"):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(new_programs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fed250-b1f5-4e15-9d10-7fdb56dda1f3",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d300821-a292-4218-b0d9-255bf911c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import enum\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# An exception to limit the maximum number of allowed transformations \n",
    "class NbTranformationException(Exception):\n",
    "    pass\n",
    "\n",
    "class RandomMatrix(Exception):\n",
    "    pass\n",
    "\n",
    "# An exception to limit the maximum number of read-write accesses. \n",
    "class NbAccessException(Exception):\n",
    "    pass\n",
    "\n",
    "# An exception to limit the maximum number of nested loops. Currently set to 5.\n",
    "class LoopsDepthException(Exception):\n",
    "    pass\n",
    "\n",
    "# An exception that's raised when we don't find the expression tree representation\n",
    "class ComputationExpressionException(Exception):\n",
    "    pass\n",
    "\n",
    "# Maximum sequence of transformations (reversal, interchange and skewing) allowed. Currently set to 5 \n",
    "MAX_NUM_TRANSFORMATIONS = 4\n",
    "\n",
    "# Maximum size of the tags vector representing each transformation\n",
    "MAX_TAGS = 8\n",
    "\n",
    "# Enumeration for the different exploration algorithms used to generate the data\n",
    "# class Exploration_method(int, enum.Enum):\n",
    "#    Beam_search = 0\n",
    "#    Recursive_beam_search = 1\n",
    "#    Reinforcement_learning = 2\n",
    "\n",
    "def get_transformations_list(\n",
    "    program_json, schedule_json, max_depth,\n",
    "):\n",
    "    \n",
    "    computations_dict = program_json[\"computations\"]\n",
    "    ordered_comp_list = sorted(\n",
    "        list(computations_dict.keys()),\n",
    "        key=lambda x: computations_dict[x][\"absolute_order\"],\n",
    "    )\n",
    "    \n",
    "    transformations_list = []\n",
    "    for comp_index, comp_name in enumerate(ordered_comp_list):\n",
    "        comp_dict = program_json[\"computations\"][comp_name]\n",
    "        comp_schedule_dict = schedule_json[comp_name]\n",
    "        \n",
    "        # Check which transformations (interchange, reversal and skweing) were applied and add the padded vector representation to their corresponding position\n",
    "        padded_tags = get_transformation_tags(\n",
    "            program_json, schedule_json, comp_name, max_depth\n",
    "        )\n",
    "        transformations_list.append(padded_tags)\n",
    "\n",
    "    return transformations_list\n",
    "\n",
    "# A function to extract the transformations applied on a spesific computation in the form of a vector of tags\n",
    "# Padding is added if the number of transformations is less than the maximum value of MAX_NUM_TRANSFORMATIONS\n",
    "# Currently our dataset represents transformations in two different formats.\n",
    "#         1- in the form of matrices from the polyhedral representation\n",
    "#         2- in the form of tags for each transformation\n",
    "# We generated a variaty of representations to test which one is more useful for our spesfici usage\n",
    "# In this function we will be unifying all of the dataset into the tags representation \n",
    "# The tag representation is as follows:\n",
    "#         ['type_of_transformation', 'first_interchange_loop', 'second_interchange_loop', 'reversed_loop', 'first_skewing_loop', 'second_skewing_loop', 'first_skew_factor', 'second_skew_factor']\n",
    "#     Where the type_of_transformation tag is:\n",
    "#         - 0 for no transformation being applied\n",
    "#         - 1 for loop interchange\n",
    "#         - 2 for loop reversal\n",
    "#         - 3 for loop skewing\n",
    "\n",
    "def get_transformation_tags(\n",
    "    program_json, schedule_json, comp_name, max_depth=None\n",
    "):\n",
    "    # Extract information about the computation and the transformations that were applied from the json input\n",
    "    comp_dict = program_json[\"computations\"][comp_name]\n",
    "    comp_schedule_dict = schedule_json[comp_name]\n",
    "    nb_iterators = len(comp_dict[\"iterators\"])\n",
    "    loop_nest = comp_dict[\"iterators\"][:]\n",
    "    \n",
    "    # Create an identity vector that represents that no transformation was applied\n",
    "    identity = np.zeros((nb_iterators, nb_iterators), int)\n",
    "    np.fill_diagonal(identity, 1)\n",
    "    identity_tags = np.zeros((1,MAX_TAGS), dtype=np.int32)\n",
    "    \n",
    "    tag_factors = []\n",
    "    \n",
    "    # If the transformations are represented using matrices\n",
    "    if \"transformation_matrices\" in comp_schedule_dict:\n",
    "        \n",
    "        if comp_schedule_dict[\"transformation_matrices\"] != []:\n",
    "            if (\"transformation_matrix\" in comp_schedule_dict) and (\n",
    "                comp_schedule_dict[\"transformation_matrix\"]\n",
    "            ):\n",
    "                # transformation_matrix represents all of the applied transformations in a single compact matrix\n",
    "                final_transformation_matrix = np.array(\n",
    "                    list(map(int, comp_schedule_dict[\"transformation_matrix\"]))\n",
    "                ).reshape(nb_iterators, nb_iterators)\n",
    "            else:\n",
    "                final_transformation_matrix = identity.copy()\n",
    "\n",
    "            \n",
    "            tag_factors = []\n",
    "            # For each transformation in the schedule\n",
    "            for matrix in comp_schedule_dict[\"transformation_matrices\"][::-1]:\n",
    "                # Check whether the size of the matrix is coherent\n",
    "                assert np.sqrt(len(matrix)) == nb_iterators\n",
    "                transformation_matrix = np.array(list(map(int, matrix))).reshape(\n",
    "                    nb_iterators, nb_iterators\n",
    "                )\n",
    "                tags_vector = identity_tags.copy()\n",
    "                # Calculate the residual to check whether this transformation is the identity matrix\n",
    "                residual = np.abs(identity - transformation_matrix)\n",
    "                \n",
    "                mask_line = residual.sum(axis=1) > 0\n",
    "                mask_col  = residual.sum(axis=0) > 0\n",
    "                \n",
    "                non_zeros_positions = np.argwhere(residual>0)\n",
    "\n",
    "                for index in non_zeros_positions:\n",
    "                    if (abs(index[0] - index[1]) > 1) and nb_iterators != abs(transformation_matrix).sum():\n",
    "\n",
    "                        raise RandomMatrix\n",
    "                    else:\n",
    "                        if (index[1] < index[0]):\n",
    "                            \n",
    "                            first_factor = transformation_matrix[index[0]-1][index[1]]\n",
    "                            second_factor = transformation_matrix[index[0]-1][index[1]+1]\n",
    "\n",
    "                            if ( index[1] < index[0] and first_factor == 1 and second_factor == 0 ):\n",
    "\n",
    "                                raise RandomMatrix\n",
    "                \n",
    "                # If that's the case, skip it to avoid adding it in the representation\n",
    "                # if (transformation_matrix == identity).all():\n",
    "                if (transformation_matrix == identity).all():\n",
    "                    continue\n",
    "                    \n",
    "                # If the transformation isn't the identity, fill the tag_vector with the corresponsonding transformation info\n",
    "                elif residual.sum() != 0 and nb_iterators == transformation_matrix.sum():\n",
    "                    # If the sum of the elements in the matrix is equal to the number of iterators, then interchange has been applied\n",
    "                    tags_vector[0][0] = 1\n",
    "                    \n",
    "                    # Extract the interchange parameters (loop indecies) through the created mask\n",
    "                    dims = np.arange((nb_iterators),dtype=int)[mask_line]\n",
    "                    \n",
    "                    assert dims.shape[0] == 2\n",
    "                    first_iter_index, second_iter_index=dims\n",
    "                    \n",
    "                    # Add the interchange parameters to the tag vector\n",
    "                    tags_vector[0][1], tags_vector[0][2] = first_iter_index, second_iter_index\n",
    "                \n",
    "                elif nb_iterators - 2 == np.trace(transformation_matrix) and transformation_matrix.sum() - np.trace(transformation_matrix) == 0:  \n",
    "                    # If the sum of the elements in the matrix is equal to the number of iterators -2, then loop reversal has been applied\n",
    "                    tags_vector[0][0] = 2      \n",
    "                    \n",
    "                    # Extract the reversed loop index\n",
    "                    dims = np.arange((nb_iterators),dtype=int)[mask_line]\n",
    "#                     print(transformation_matrix)\n",
    "                    assert dims.shape[0] == 1\n",
    "                    index=dims[0]\n",
    "                    \n",
    "                    # Add the reversal parameter to the tags vector\n",
    "                    tags_vector[0][3] = index\n",
    "                else:\n",
    "                    # If the matrix isn't one of the first three cases (identity, reversal, interchange) then we know it represents skewing\n",
    "                    tags_vector[0][0] = 3\n",
    "                    dims_line = np.arange((nb_iterators),dtype=int)[mask_line]\n",
    "                    dims_col = np.arange((nb_iterators),dtype=int)[mask_col]\n",
    "                    \n",
    "                    dims_line = [dims_line[0], dims_line[0]+1]\n",
    "                    \n",
    "                    dims_col = [dims_col[0], dims_col[0]+1]\n",
    "                    \n",
    "                    # Extract which loops have been skewed \n",
    "                    first_iter_index, second_iter_index = dims_line\n",
    "                    \n",
    "                    # Add the skewed loops indecies to the tags vector\n",
    "                    tags_vector[0][4], tags_vector[0][5] = first_iter_index, second_iter_index\n",
    "\n",
    "                    \n",
    "                    # Extract the skewing factors \n",
    "                    first_factor = transformation_matrix[first_iter_index, first_iter_index]\n",
    "                    second_factor = transformation_matrix[first_iter_index, second_iter_index]\n",
    "                    a = transformation_matrix[second_iter_index, first_iter_index]\n",
    "                    b = transformation_matrix[second_iter_index, second_iter_index]\n",
    "                    \n",
    "        \n",
    "                    if ((a, b) != linear_diophantine_default(first_factor, second_factor)):\n",
    "\n",
    "                        raise RandomMatrix\n",
    "                    \n",
    "                    # Add the skewing factors to the tags vector\n",
    "                    tags_vector[0][6], tags_vector[0][7] = first_factor, second_factor\n",
    "                \n",
    "                transformation_matrix = np.c_[\n",
    "                    np.ones(transformation_matrix.shape[0]), transformation_matrix\n",
    "                ]\n",
    "                transformation_matrix = np.r_[\n",
    "                    [np.ones(transformation_matrix.shape[1])], transformation_matrix\n",
    "                ]\n",
    "                transformation_matrix = np.pad(\n",
    "                    transformation_matrix,\n",
    "                    [\n",
    "                        (0, max_depth + 1 - transformation_matrix.shape[0]),\n",
    "                        (0, max_depth + 1 - transformation_matrix.shape[1]),\n",
    "                    ],\n",
    "                    mode=\"constant\",\n",
    "                    constant_values=0,\n",
    "                )\n",
    "                tag_factors.append(tags_vector)\n",
    "            \n",
    "            # We use MAX_NUM_TRANSFORMATIONS+1 instead of MAX_NUM_TRANSFORMATIONS to include the matrix that represents the whole sequence\n",
    "            if len(tag_factors) > (MAX_NUM_TRANSFORMATIONS):\n",
    "                # If the number of transformations is greater than the maximum allowed, raise an exception\n",
    "                raise NbTranformationException\n",
    "    \n",
    "    \n",
    "            # If no tranformation was found (comp_schedule_dict[\"transformation_matrices\"] only contains the identity) then we use the idendity tags\n",
    "            final_tags = (\n",
    "                np.concatenate(tag_factors, axis=0)\n",
    "                if tag_factors\n",
    "                else identity_tags.copy()\n",
    "            )\n",
    "        else:\n",
    "            # If comp_schedule_dict[\"transformation_matrices\"] is empty we also use the identity tags \n",
    "            final_transformation_matrix = identity.copy()\n",
    "            final_tags = identity_tags.copy()\n",
    "        # To make sure the data is coherent, we want to check whether the sequence of transformations is equal to the final transformation matrix (represented by final_transformation_matrix) \n",
    "        # In the polyhedral model, the multiplication of the sequence of transformation matrices results in one matrix that represents the whole combination\n",
    "        comparison_matrix = identity.copy()\n",
    "        for mat in comp_schedule_dict[\"transformation_matrices\"][::-1]:\n",
    "            comparison_matrix = comparison_matrix @ np.array(\n",
    "                list(map(int, mat))\n",
    "            ).reshape(nb_iterators, nb_iterators)\n",
    "        \n",
    "        assert (comparison_matrix == final_transformation_matrix).all()\n",
    "    else:\n",
    "        # If the non-polyhedral representation is used, only interchange and skewing are applied\n",
    "        # The skewing and interchange parameters are present in the json\n",
    "        # We directly add them to the tags vector \n",
    "        if comp_schedule_dict[\"interchange_dims\"]:\n",
    "            first_iter_index = loop_nest.index(\n",
    "                comp_schedule_dict[\"interchange_dims\"][0]\n",
    "            )\n",
    "            second_iter_index = loop_nest.index(\n",
    "                comp_schedule_dict[\"interchange_dims\"][1]\n",
    "            )\n",
    "            loop_nest[first_iter_index], loop_nest[second_iter_index] = (\n",
    "                loop_nest[second_iter_index],\n",
    "                loop_nest[first_iter_index],\n",
    "            )\n",
    "            tags_vector = identity_tags.copy()\n",
    "            tags_vector[0][0] = 1\n",
    "            tags_vector[0][1], tags_vector[0][2] = first_iter_index, second_iter_index\n",
    "            tag_factors.append(tags_vector)\n",
    "\n",
    "        \n",
    "        if comp_schedule_dict[\"skewing\"]:\n",
    "            first_iter_index = loop_nest.index(\n",
    "                comp_schedule_dict[\"skewing\"][\"skewed_dims\"][0]\n",
    "            )\n",
    "            second_iter_index = loop_nest.index(\n",
    "                comp_schedule_dict[\"skewing\"][\"skewed_dims\"][1]\n",
    "            )\n",
    "            first_factor = int(comp_schedule_dict[\"skewing\"][\"skewing_factors\"][0])\n",
    "            second_factor = int(comp_schedule_dict[\"skewing\"][\"skewing_factors\"][1])\n",
    "\n",
    "            tags_vector = identity_tags.copy()\n",
    "            tags_vector[0][0] = 3\n",
    "            tags_vector[0][4], tags_vector[0][5] = first_iter_index, second_iter_index\n",
    "            tags_vector[0][6], tags_vector[0][7] = first_factor, second_factor\n",
    "            tag_factors.append(tags_vector)\n",
    "        # If neither skewing or interchange were applied, we use the identity tags vector\n",
    "        final_tags = (\n",
    "            np.concatenate(tag_factors, axis=0)\n",
    "            if tag_factors\n",
    "            else identity_tags.copy()\n",
    "        )\n",
    "        \n",
    "    return final_tags\n",
    "# returns a string representation of a schedule and the transformations applied in it\n",
    "def get_schedule_str(program_json, sched_json):\n",
    "    comp_name = [\n",
    "        n\n",
    "        for n in sched_json.keys()\n",
    "        if not n in [\"unfuse_iterators\", \"tree_structure\", \"execution_times\", \"fusions\", \"sched_str\", \"legality_check\", \"exploration_method\"]\n",
    "    ]\n",
    "    sched_str = \"\"\n",
    "    \n",
    "    if (\"fusions\" in sched_json and sched_json[\"fusions\"]):\n",
    "        for fusion in sched_json[\"fusions\"]:\n",
    "            sched_str += \"F(\"\n",
    "            for name in comp_name:\n",
    "                if name in fusion:\n",
    "                    sched_str += name + \",\"\n",
    "            \n",
    "            sched_str = sched_str[:-1]\n",
    "            sched_str += \")\"\n",
    "            \n",
    "    for name in comp_name:\n",
    "        transf_loop_nest = program_json[\"computations\"][name][\"iterators\"].copy()\n",
    "        schedule = sched_json[name]\n",
    "        sched_str += '{' + name + '}:'\n",
    "\n",
    "        for transformation in schedule[\"transformations_list\"]:\n",
    "\n",
    "            if (transformation[0] == 1):\n",
    "                sched_str += \"I(L\" + str(transformation[1]) + \",L\" + str(transformation[2]) + \")\"\n",
    "                \n",
    "            elif (transformation[0] == 2):\n",
    "                sched_str += \"R(L\" + str(transformation[3])+ \")\"\n",
    "            elif (transformation[0] == 3):\n",
    "                sched_str += \"S(L\" + str(transformation[4]) + \",L\" + str(transformation[5]) + \",\" + str(transformation[6]) + \",\" + str(transformation[7]) + \")\"\n",
    "                \n",
    "        if schedule[\"parallelized_dim\"]:\n",
    "            \n",
    "            dim_index = transf_loop_nest.index(schedule[\"parallelized_dim\"])\n",
    "            sched_str += \"P(L\" + str(dim_index) + \")\"\n",
    "\n",
    "        if schedule[\"tiling\"]:\n",
    "            if schedule[\"tiling\"][\"tiling_depth\"] == 2:\n",
    "                first_dim = schedule[\"tiling\"][\"tiling_dims\"][0]\n",
    "                second_dim = schedule[\"tiling\"][\"tiling_dims\"][1]\n",
    "                \n",
    "                first_dim_index = transf_loop_nest.index(first_dim)\n",
    "                second_dim_index = transf_loop_nest.index(second_dim)\n",
    "                first_factor = schedule[\"tiling\"][\"tiling_factors\"][0]\n",
    "                second_factor = schedule[\"tiling\"][\"tiling_factors\"][1]\n",
    "                sched_str += (\n",
    "                    \"T2(L\"\n",
    "                    + str(first_dim_index)\n",
    "                    + \",L\"\n",
    "                    + str(second_dim_index)\n",
    "                    + \",\"\n",
    "                    + str(first_factor)\n",
    "                    + \",\"\n",
    "                    + str(second_factor)\n",
    "                    + \")\"\n",
    "                )\n",
    "                i = transf_loop_nest.index(first_dim)\n",
    "                transf_loop_nest[i : i + 1] = first_dim + \"_outer\", second_dim + \"_outer\"\n",
    "                i = transf_loop_nest.index(second_dim)\n",
    "                transf_loop_nest[i : i + 1] = first_dim + \"_inner\", second_dim + \"_inner\"\n",
    "            else:\n",
    "                first_dim = schedule[\"tiling\"][\"tiling_dims\"][0]\n",
    "                second_dim = schedule[\"tiling\"][\"tiling_dims\"][1]\n",
    "                third_dim = schedule[\"tiling\"][\"tiling_dims\"][2]\n",
    "                first_dim_index = transf_loop_nest.index(first_dim)\n",
    "                second_dim_index = transf_loop_nest.index(second_dim)\n",
    "                third_dim_index = transf_loop_nest.index(third_dim)\n",
    "                first_factor = schedule[\"tiling\"][\"tiling_factors\"][0]\n",
    "                second_factor = schedule[\"tiling\"][\"tiling_factors\"][1]\n",
    "                third_factor = schedule[\"tiling\"][\"tiling_factors\"][2]\n",
    "                sched_str += (\n",
    "                    \"T3(L\"\n",
    "                    + str(first_dim_index)\n",
    "                    + \",L\"\n",
    "                    + str(second_dim_index)\n",
    "                    + \",L\"\n",
    "                    + str(third_dim_index)\n",
    "                    + \",\"\n",
    "                    + str(first_factor)\n",
    "                    + \",\"\n",
    "                    + str(second_factor)\n",
    "                    + \",\"\n",
    "                    + str(third_factor)\n",
    "                    + \")\"\n",
    "                )\n",
    "                i = transf_loop_nest.index(first_dim)\n",
    "                transf_loop_nest[i : i + 1] = (\n",
    "                    first_dim + \"_outer\",\n",
    "                    second_dim + \"_outer\",\n",
    "                    third_dim + \"_outer\",\n",
    "                )\n",
    "                i = transf_loop_nest.index(second_dim)\n",
    "                transf_loop_nest[i : i + 1] = (\n",
    "                    first_dim + \"_inner\",\n",
    "                    second_dim + \"_inner\",\n",
    "                    third_dim + \"_inner\",\n",
    "                )\n",
    "                transf_loop_nest.remove(third_dim)\n",
    "\n",
    "        if schedule[\"unrolling_factor\"]:\n",
    "            dim_index = len(transf_loop_nest) - 1\n",
    "            dim_name = transf_loop_nest[-1]\n",
    "            sched_str += \"U(L\" + str(dim_index) + \",\" + schedule[\"unrolling_factor\"] + \")\"\n",
    "            transf_loop_nest[dim_index : dim_index + 1] = (\n",
    "                dim_name + \"_Uouter\",\n",
    "                dim_name + \"_Uinner\",\n",
    "            )\n",
    "    return sched_str\n",
    "\n",
    "# Creates a template for the input representation\n",
    "def check_program_access_and_depth(program_dict, max_depth):\n",
    "    # Set the max and min number of accesses allowed \n",
    "    max_accesses = 15\n",
    "    min_accesses = 1\n",
    "\n",
    "    # Get the program JSON represenation\n",
    "    program_json = program_dict[\"program_annotation\"]\n",
    "    \n",
    "    # Get the computations (program statements) dictionary and order them according to the absolute_order attribute\n",
    "    computations_dict = program_json[\"computations\"]\n",
    "    ordered_comp_list = sorted(\n",
    "        list(computations_dict.keys()),\n",
    "        key=lambda x: computations_dict[x][\"absolute_order\"],\n",
    "    )\n",
    "\n",
    "    for comp_index, comp_name in enumerate(ordered_comp_list):\n",
    "        \n",
    "        comp_dict = computations_dict[comp_name]\n",
    "        \n",
    "        if len(comp_dict[\"accesses\"]) > max_accesses:\n",
    "            raise NbAccessException\n",
    "        \n",
    "        if len(comp_dict[\"accesses\"]) < min_accesses:\n",
    "            raise NbAccessException\n",
    "        \n",
    "        if len(comp_dict[\"iterators\"]) > max_depth:\n",
    "            raise LoopsDepthException\n",
    "    \n",
    "    return\n",
    "\n",
    "def has_skippable_loop_1comp(\n",
    "    prog_dict,\n",
    "):\n",
    "\n",
    "    program_json = prog_dict[\"program_annotation\"]\n",
    "    if not len(program_json[\"computations\"]) == 1:\n",
    "        return False\n",
    "    comp_name = list(program_json[\"computations\"].keys())[0]\n",
    "    comp_dict = program_json[\"computations\"][comp_name]\n",
    "    write_buffer_id = comp_dict[\"write_buffer_id\"]\n",
    "    iterators = comp_dict[\"iterators\"]\n",
    "    write_dims = isl_to_write_dims(comp_dict[\"write_access_relation\"])\n",
    "    read_buffer_ids = [e[\"buffer_id\"] for e in comp_dict[\"accesses\"]]\n",
    "\n",
    "    if len(write_dims) == len(iterators):\n",
    "\n",
    "        if (\n",
    "            len(read_buffer_ids) == 1\n",
    "            and read_buffer_ids[0] == write_buffer_id\n",
    "            and comp_dict[\"number_of_additions\"] == 0\n",
    "            and comp_dict[\"number_of_subtraction\"] == 0\n",
    "            and comp_dict[\"number_of_multiplication\"] == 0\n",
    "            and comp_dict[\"number_of_division\"] == 0\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    if not write_buffer_id in read_buffer_ids:\n",
    "        return True\n",
    "\n",
    "    found = False\n",
    "    for access in comp_dict[\"accesses\"]:\n",
    "        if access[\"buffer_id\"] == write_buffer_id and not access_is_stencil(access):\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        if write_dims[-1] != iterators[-1]:\n",
    "            return True\n",
    "\n",
    "    for access in comp_dict[\"accesses\"]:\n",
    "        if access[\"buffer_id\"] == write_buffer_id and access_is_stencil(access):\n",
    "            return False\n",
    "\n",
    "    read_dims_bools = []\n",
    "    for access in comp_dict[\"accesses\"]:\n",
    "        read_dims_bools.append(np.any(access[\"access_matrix\"], axis=0))\n",
    "    read_dims_bools = np.any(read_dims_bools, axis=0)\n",
    "    read_iterators = [\n",
    "        iterators[i]\n",
    "        for i, is_used in enumerate(read_dims_bools[:-1])\n",
    "        if is_used == True\n",
    "    ]\n",
    "    used_iterators = set(write_dims + read_iterators)\n",
    "    if len(used_iterators) == len(iterators):\n",
    "        return False\n",
    "\n",
    "    if iterators[-1] in used_iterators:\n",
    "        if len(comp_dict[\"accesses\"]) > 2:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def drop_program(prog_dict, prog_name):\n",
    "#     print(prog_name[8:])\n",
    "    if len(prog_dict[\"schedules_list\"]) < 2:\n",
    "        return True\n",
    "    if ( 750000 <= int(prog_name[8:]) and int(prog_name[8:])<=752600 ):\n",
    "    \n",
    "#         print(\"Found an random matrix program\", prog_name)\n",
    "        return True\n",
    "    if has_skippable_loop_1comp(prog_dict):\n",
    "        return True\n",
    "    if (\n",
    "        \"node_name\" in prog_dict and prog_dict[\"node_name\"] == \"lanka24\"\n",
    "    ):  # drop if we the program is run by lanka24 (because its measurements are inacurate)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def drop_schedule(prog_dict, schedule_index):\n",
    "    schedule_json = prog_dict[\"schedules_list\"][schedule_index]\n",
    "    if (not schedule_json[\"execution_times\"]) or min(\n",
    "        schedule_json[\"execution_times\"]\n",
    "    ) < 0:  # exec time is set to -1 on datapoints that are deemed noisy, or if list empty\n",
    "        return True\n",
    "    if sched_is_prunable(prog_dict[\"program_annotation\"], schedule_json):\n",
    "            return True\n",
    "    if wrongly_set_to_default_schedule(prog_dict, schedule_index):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def speedup_clip(speedup):\n",
    "    if speedup < 0.01:\n",
    "        speedup = 0.01\n",
    "    return speedup\n",
    "\n",
    "# TODO what does this function do\n",
    "def isl_to_write_dims(\n",
    "    isl_map,\n",
    "):\n",
    "    buffer_iterators_str = re.findall(r\"->\\s*\\w*\\[(.*)\\]\", isl_map)[0]\n",
    "    buffer_iterators_str = re.sub(r\"\\w+'\\s=\", \"\", buffer_iterators_str)\n",
    "    buf_iter_names = re.findall(r\"(?:\\s*(\\w+))+\", buffer_iterators_str)\n",
    "    return buf_iter_names\n",
    "\n",
    "# returns a string representation of a schedule and the transformations applied in it\n",
    "\n",
    "def wrongly_set_to_default_schedule(program_dict, schedule_index):\n",
    "    \n",
    "    schedule_dict = program_dict[\"schedules_list\"][schedule_index]\n",
    "    if len(schedule_dict[\"execution_times\"]) == 1:\n",
    "        speed_up = program_dict[\"initial_execution_time\"] / schedule_dict[\"execution_times\"][0]\n",
    "        \n",
    "        if (speed_up > 0.00099 and speed_up < 0.00101) and (not can_set_default_eval(program_dict[\"program_annotation\"], schedule_dict)):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def access_is_stencil(access):\n",
    "    return np.any(access[\"access_matrix\"], axis=0)[-1]\n",
    "\n",
    "# Solving the Linear Diophantine equation & finding basic solution (sigma & gamma) for : f_i* sigma - f_j*gamma = 1\n",
    "# Used to get skewing parameters\n",
    "def linear_diophantine_default(f_i, f_j):\n",
    "    n1 = abs(f_i)\n",
    "    n2 = abs(f_j)\n",
    "    \n",
    "    while(n1 != n2):\n",
    "        if(n1 > n2):\n",
    "            n1 -=  n2\n",
    "        else:\n",
    "            n2 -=  n1\n",
    "            \n",
    "    # Update f_i and f_j to equivalent but prime between themselfs value\n",
    "    f_i = f_i / n1\n",
    "    f_j = f_j / n1\n",
    "    \n",
    "    found = False\n",
    "    gamma = 0\n",
    "    sigma = 1\n",
    "    \n",
    "    if (f_j == 1) or (f_i == 1):\n",
    "        gamma = f_i - 1\n",
    "        sigma = 1\n",
    "        # Since sigma = 1  then\n",
    "        # f_i - gamma * f_j = 1 & using the previous condition :\n",
    "        #  - f_i = 1 : then gamma = 0 (f_i-1) is enough\n",
    "        #  - f_j = 1 : then gamma = f_i -1  \n",
    "    else:\n",
    "        if (f_j == -1) and (f_i > 1):\n",
    "            gamma = 1\n",
    "            sigma = 0\n",
    "        else:\n",
    "            # General case : solving the Linear Diophantine equation & finding basic solution (sigma & gamma) for : f_i* sigma - f_j*gamma = 1\n",
    "            i = 0\n",
    "            while (i < 100) and (not found):\n",
    "                if ((sigma * f_i) % abs(f_j)) == 1:\n",
    "                    found = True\n",
    "                else:\n",
    "                    sigma += 1\n",
    "                    i += 1\n",
    "            if not found:\n",
    "                # Detect infinite loop and prevent it in case where f_i and f_j are not prime between themselfs\n",
    "                print(\"Error cannof find solution to diophantine equation\")\n",
    "                return\n",
    "            gamma = ((sigma * f_i) - 1) / f_j\n",
    "    return gamma, sigma\n",
    "\n",
    "def sched_is_prunable(program_json, schedule_json):\n",
    "    reg = \"\"\n",
    "    comp_names = [\n",
    "        n\n",
    "        for n in schedule_json.keys()\n",
    "        if not n in [\"unfuse_iterators\", \"tree_structure\", \"execution_times\", \"fusions\", \"sched_str\", \"legality_check\", \"exploration_method\"]\n",
    "    ]\n",
    "    for name in comp_names:\n",
    "        innermost_loop = len(program_json[\"computations\"][name][\"iterators\"])-1\n",
    "        reg += f\"{{{name}}}:P\\(L{innermost_loop}\\)U.*|\"\n",
    "    if(len(comp_names)>0):\n",
    "        reg = reg[:-1]\n",
    "    \n",
    "    schedule_str = get_schedule_str_for_pruning(program_json, schedule_json)\n",
    "    if re.search(reg, schedule_str):\n",
    "        return True\n",
    "    reg = \"\"\n",
    "    \n",
    "    for name in comp_names:\n",
    "        innermost_loop = len(program_json[\"computations\"][name][\"iterators\"])-1\n",
    "        reg += f\"{{{name}}}:P\\(L{innermost_loop}\\)T2\\(L{innermost_loop-2},L{innermost_loop-1}.*|\"\n",
    "    \n",
    "    if(len(comp_names)>0):\n",
    "        reg = reg[:-1]\n",
    "    if re.search(reg, schedule_str):\n",
    "        return True                                                                                                                               \n",
    "    return False\n",
    "\n",
    "def can_set_default_eval(program_json, schedule_json):\n",
    "    reg = \"\"\n",
    "    comp_names = [\n",
    "        n\n",
    "        for n in schedule_json.keys()\n",
    "        if not n in [\"unfuse_iterators\", \"tree_structure\", \"execution_times\", \"fusions\", \"sched_str\", \"legality_check\", \"exploration_method\"]\n",
    "    ]\n",
    "    for name in comp_names:\n",
    "        innermost_loop = len(program_json[\"computations\"][name][\"iterators\"])-1\n",
    "        reg += f\"{{{name}}}:P\\(L{innermost_loop}\\)({{|$)|\"\n",
    "    if(len(comp_names)>0):\n",
    "        reg = reg[:-1]\n",
    "    \n",
    "    schedule_str = get_schedule_str_for_pruning(program_json, schedule_json)\n",
    "    if re.search(reg, schedule_str):    \n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "def get_schedule_str_for_pruning(program_json, sched_json):\n",
    "    comp_name = [\n",
    "        n\n",
    "        for n in sched_json.keys()\n",
    "        if not n in [\"unfuse_iterators\", \"tree_structure\", \"execution_times\", \"fusions\", \"sched_str\", \"legality_check\", \"exploration_method\"]\n",
    "    ]\n",
    "    sched_str = \"\"\n",
    "#     print(f\"starting for new schedules in program: {program_json}\")\n",
    "    for name in comp_name:\n",
    "        # can probably use the feature in prog json\n",
    "        transf_loop_nest = program_json[\"computations\"][name][\"iterators\"].copy()\n",
    "        schedule = sched_json[name]\n",
    "        sched_str += '{' + name + '}:'\n",
    "#         print(f\"transf_loop_nest for computation: {name} is {transf_loop_nest}\")\n",
    "        if schedule[\"parallelized_dim\"]:\n",
    "            \n",
    "            dim_index = transf_loop_nest.index(schedule[\"parallelized_dim\"])\n",
    "            sched_str += \"P(L\" + str(dim_index) + \")\"\n",
    "\n",
    "        if schedule[\"tiling\"]:\n",
    "            if schedule[\"tiling\"][\"tiling_depth\"] == 2:\n",
    "                first_dim = schedule[\"tiling\"][\"tiling_dims\"][0]\n",
    "                second_dim = schedule[\"tiling\"][\"tiling_dims\"][1]\n",
    "                \n",
    "                first_dim_index = transf_loop_nest.index(first_dim)\n",
    "                second_dim_index = transf_loop_nest.index(second_dim)\n",
    "                first_factor = schedule[\"tiling\"][\"tiling_factors\"][0]\n",
    "                second_factor = schedule[\"tiling\"][\"tiling_factors\"][1]\n",
    "                sched_str += (\n",
    "                    \"T2(L\"\n",
    "                    + str(first_dim_index)\n",
    "                    + \",L\"\n",
    "                    + str(second_dim_index)\n",
    "                    + \",\"\n",
    "                    + str(first_factor)\n",
    "                    + \",\"\n",
    "                    + str(second_factor)\n",
    "                    + \")\"\n",
    "                )\n",
    "                i = transf_loop_nest.index(first_dim)\n",
    "                transf_loop_nest[i : i + 1] = first_dim + \"_outer\", second_dim + \"_outer\"\n",
    "                i = transf_loop_nest.index(second_dim)\n",
    "                transf_loop_nest[i : i + 1] = first_dim + \"_inner\", second_dim + \"_inner\"\n",
    "            else:\n",
    "                first_dim = schedule[\"tiling\"][\"tiling_dims\"][0]\n",
    "                second_dim = schedule[\"tiling\"][\"tiling_dims\"][1]\n",
    "                third_dim = schedule[\"tiling\"][\"tiling_dims\"][2]\n",
    "                first_dim_index = transf_loop_nest.index(first_dim)\n",
    "                second_dim_index = transf_loop_nest.index(second_dim)\n",
    "                third_dim_index = transf_loop_nest.index(third_dim)\n",
    "                first_factor = schedule[\"tiling\"][\"tiling_factors\"][0]\n",
    "                second_factor = schedule[\"tiling\"][\"tiling_factors\"][1]\n",
    "                third_factor = schedule[\"tiling\"][\"tiling_factors\"][2]\n",
    "                sched_str += (\n",
    "                    \"T3(L\"\n",
    "                    + str(first_dim_index)\n",
    "                    + \",L\"\n",
    "                    + str(second_dim_index)\n",
    "                    + \",L\"\n",
    "                    + str(third_dim_index)\n",
    "                    + \",\"\n",
    "                    + str(first_factor)\n",
    "                    + \",\"\n",
    "                    + str(second_factor)\n",
    "                    + \",\"\n",
    "                    + str(third_factor)\n",
    "                    + \")\"\n",
    "                )\n",
    "                i = transf_loop_nest.index(first_dim)\n",
    "                transf_loop_nest[i : i + 1] = (\n",
    "                    first_dim + \"_outer\",\n",
    "                    second_dim + \"_outer\",\n",
    "                    third_dim + \"_outer\",\n",
    "                )\n",
    "                i = transf_loop_nest.index(second_dim)\n",
    "                transf_loop_nest[i : i + 1] = (\n",
    "                    first_dim + \"_inner\",\n",
    "                    second_dim + \"_inner\",\n",
    "                    third_dim + \"_inner\",\n",
    "                )\n",
    "                transf_loop_nest.remove(third_dim)\n",
    "\n",
    "        if schedule[\"unrolling_factor\"]:\n",
    "            dim_index = len(transf_loop_nest) - 1\n",
    "            dim_name = transf_loop_nest[-1]\n",
    "            sched_str += \"U(L\" + str(dim_index) + \",\" + schedule[\"unrolling_factor\"] + \")\"\n",
    "            transf_loop_nest[dim_index : dim_index + 1] = (\n",
    "                dim_name + \"_Uouter\",\n",
    "                dim_name + \"_Uinner\",\n",
    "            )\n",
    "    return sched_str\n",
    "    \n",
    "def add_expression_representation_to_function(program_json, function_name):\n",
    "    # Checking if it already contains the expression representation\n",
    "    if 'expression_representation' in program_json[\"computations\"][list(program_json[\"computations\"].keys())[0]]:\n",
    "        return program_json\n",
    "    \n",
    "    # Looking for the function expressions file\n",
    "    i=0\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/5x_functions/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/depricated/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/done/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/done_mats/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/done_mats_2comps/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/done_mats_gnrl_mcomp/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    try:\n",
    "        exprs_file = pd.read_json(\"/data/mk8958/exprs_jsons/filtered_function602486/\"+function_name+\"_expr_representation.json\")\n",
    "    except:\n",
    "        i+=1\n",
    "    if i == 7 :\n",
    "        raise ComputationExpressionException\n",
    "    \n",
    "    # Adding the expression representation to each computation\n",
    "    for comp in program_json[\"computations\"]:\n",
    "        if exprs_file[function_name+\"_explored_schedules.json\"][\"computations\"][comp] is not None:\n",
    "            program_json[\"computations\"][comp][\"expression_representation\"] = exprs_file[function_name+\"_explored_schedules.json\"][\"computations\"][comp]\n",
    "        else:\n",
    "            raise ComputationExpressionException\n",
    "    return program_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0d5b6-cada-4bae-b93b-838baddcc2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
